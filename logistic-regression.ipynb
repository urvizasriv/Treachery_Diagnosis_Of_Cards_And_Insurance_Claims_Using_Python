{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd # used as operations for manipulating numerical tables and time series.\nidentity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')   #reading identity data\ntransaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')  #reading the transaction data\nimport matplotlib.pyplot as plt #creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.\nimport seaborn as sns #helps you explore and understand your data.\nimport numpy as np","metadata":{"id":"hZS55QiB9yY6","outputId":"5bb9a2f6-d701-4cc8-af2e-f770db742228","execution":{"iopub.status.busy":"2022-04-28T15:26:44.443821Z","iopub.execute_input":"2022-04-28T15:26:44.444166Z","iopub.status.idle":"2022-04-28T15:27:09.177182Z","shell.execute_reply.started":"2022-04-28T15:26:44.444082Z","shell.execute_reply":"2022-04-28T15:27:09.176428Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#function to reduce the memory of dataset\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"id":"Wtize2n1-Xui","execution":{"iopub.status.busy":"2022-04-28T15:27:09.178564Z","iopub.execute_input":"2022-04-28T15:27:09.178758Z","iopub.status.idle":"2022-04-28T15:27:09.191640Z","shell.execute_reply.started":"2022-04-28T15:27:09.178733Z","shell.execute_reply":"2022-04-28T15:27:09.190331Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# ***Working with treachery diagnosys of all types of money cards***","metadata":{}},{"cell_type":"markdown","source":"**INPUT DATA:**\n\nThe data is broken in two data points’ **identity** and **transaction** which are joined by *TransactionID*.\n\n1.**Identity Table**: (Information about cards and client details)\n\n> The Identity Table has 144233 rows and 41 columns.\n\n> Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc.) and digital signature (UA/browser/so/version, etc.) associated with transactions.\n\n> Features:\n\n*         DeviceType \n*         DeviceInfo \n*         Id12-id38 \n*         TransactionID etc.\n\n2.**Transaction Table:** (Information about the transaction of the clients)\n\n> “It contains money transfer and also other gifting goods and services, like you booked a ticket for others, etc.”\n\n> Transaction table has 590540 rows and 394 columns.\n\n> Features:\n\n* ProductCD \n* isFraud \n* Card1 – card6 \n* TransactionID \n* addr1,addr2 \n* P_emaildomain \n* R_emaildomain\n\n> We have to merge the identity and transaction data points for our training\n\n**Output :**\nIn the dataset the target variable is a binary attribute ‘isFraud’ ‘0’ or ‘1’ (“Yes” or “NO”). We have to diagnose the probability that the online transaction is treacherous.\n","metadata":{}},{"cell_type":"markdown","source":"> \n**Let's first check identity table**","metadata":{}},{"cell_type":"code","source":"identity.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.192703Z","iopub.execute_input":"2022-04-28T15:27:09.192929Z","iopub.status.idle":"2022-04-28T15:27:09.238621Z","shell.execute_reply.started":"2022-04-28T15:27:09.192876Z","shell.execute_reply":"2022-04-28T15:27:09.238016Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"identity.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.240300Z","iopub.execute_input":"2022-04-28T15:27:09.240666Z","iopub.status.idle":"2022-04-28T15:27:09.352478Z","shell.execute_reply.started":"2022-04-28T15:27:09.240631Z","shell.execute_reply":"2022-04-28T15:27:09.351780Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"identity.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.353396Z","iopub.execute_input":"2022-04-28T15:27:09.353583Z","iopub.status.idle":"2022-04-28T15:27:09.359839Z","shell.execute_reply.started":"2022-04-28T15:27:09.353559Z","shell.execute_reply":"2022-04-28T15:27:09.358927Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the above analysis that identity table have 144233 rows and 41 columns ","metadata":{}},{"cell_type":"markdown","source":"**Let's check transactions table**","metadata":{}},{"cell_type":"code","source":"transaction.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.360737Z","iopub.execute_input":"2022-04-28T15:27:09.361025Z","iopub.status.idle":"2022-04-28T15:27:09.394305Z","shell.execute_reply.started":"2022-04-28T15:27:09.360998Z","shell.execute_reply":"2022-04-28T15:27:09.393544Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transaction.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.395335Z","iopub.execute_input":"2022-04-28T15:27:09.395502Z","iopub.status.idle":"2022-04-28T15:27:09.401244Z","shell.execute_reply.started":"2022-04-28T15:27:09.395481Z","shell.execute_reply":"2022-04-28T15:27:09.400058Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the above analysis that transaction table have 590540 rows and 394 columns ","metadata":{}},{"cell_type":"code","source":"#checking for the number of unique values in each column of identity table\nidentity.apply(lambda x: x.nunique()) ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.402476Z","iopub.execute_input":"2022-04-28T15:27:09.402732Z","iopub.status.idle":"2022-04-28T15:27:09.562795Z","shell.execute_reply.started":"2022-04-28T15:27:09.402704Z","shell.execute_reply":"2022-04-28T15:27:09.562204Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Merging transaction and identity data into training data**","metadata":{}},{"cell_type":"code","source":"#merging transaction and identity data by using only keys from left dataframe\n#in which primry key acts as TransactionId which is common in both tables, similar to a SQL left outer join.\ntraining = transaction.merge(identity, how = 'left')  ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:09.563702Z","iopub.execute_input":"2022-04-28T15:27:09.563926Z","iopub.status.idle":"2022-04-28T15:27:16.861410Z","shell.execute_reply.started":"2022-04-28T15:27:09.563851Z","shell.execute_reply":"2022-04-28T15:27:16.860615Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"training.head()","metadata":{"id":"xq9u4JQVBVun","execution":{"iopub.status.busy":"2022-04-28T15:27:16.863675Z","iopub.execute_input":"2022-04-28T15:27:16.864056Z","iopub.status.idle":"2022-04-28T15:27:16.887446Z","shell.execute_reply.started":"2022-04-28T15:27:16.864018Z","shell.execute_reply":"2022-04-28T15:27:16.886424Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#plotting bargraph for checking the count of transactions that are fraudulent and non-fraudulent\ntraining.groupby('isFraud').count()['TransactionID'].plot(kind='bar',\n          title='Distribution of Target in Train',color=('pink','purple'),                            \n          figsize=(8, 5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:16.889026Z","iopub.execute_input":"2022-04-28T15:27:16.889574Z","iopub.status.idle":"2022-04-28T15:27:20.079160Z","shell.execute_reply.started":"2022-04-28T15:27:16.889535Z","shell.execute_reply":"2022-04-28T15:27:20.078358Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### In above graph we can see that the fraud cases are very less as compared to non-fraud cases.","metadata":{}},{"cell_type":"code","source":"# loc locates all data by column or conditional statement\nnon_legit = training.loc[training['isFraud'] == 1] # find all rows that are fraudulent\nprint('Non legit are', len(non_legit), ' transactions or ', round(training['isFraud'].value_counts()[1]/len(training)*100, 2), '% of the dataset')\n\nlegit = training.loc[training['isFraud'] == 0] # final all rows that aren't fraudulent\nprint('Legit are ', len(legit), ' transactions or ', round(training['isFraud'].value_counts()[0]/len(training)*100, 2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:20.080187Z","iopub.execute_input":"2022-04-28T15:27:20.080483Z","iopub.status.idle":"2022-04-28T15:27:22.229307Z","shell.execute_reply.started":"2022-04-28T15:27:20.080457Z","shell.execute_reply":"2022-04-28T15:27:22.228052Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We see that about 96.5% of transactions are Non-Fraud and only 3.5% are fraud cases. There is high class imbalance in our dataset that is going to effect the validation and evaluation strategy that we are going to choose.\n\nAs the class is highly imbalanced we will be using roc_auc_score(AUC measures how well a model is able to distinguish between classes) as our evaluation metric.","metadata":{}},{"cell_type":"markdown","source":"# **Data Cleaning**","metadata":{}},{"cell_type":"markdown","source":"Those features which had missing values more than 90% were removed from the dataset because they did not provide any information to our model and they are just garbage which will affect our model’s run time.\n\nWe filled the missing values with mode values for categorical features and with median for continuous features.","metadata":{}},{"cell_type":"code","source":"training.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:22.231263Z","iopub.execute_input":"2022-04-28T15:27:22.231500Z","iopub.status.idle":"2022-04-28T15:27:23.143032Z","shell.execute_reply.started":"2022-04-28T15:27:22.231473Z","shell.execute_reply":"2022-04-28T15:27:23.141843Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"id_36             449555\nid_37             449555\nid_38             449555\nDeviceType        449730\nDeviceInfo        471874\nwe can see here that there are very high null values so these columns are of no use so we'll drop them ","metadata":{}},{"cell_type":"code","source":"null_columns = [col for col in training.columns if training[col].isnull().sum() / training.shape[0] > 0.9]\ntraining.drop(null_columns,axis=1,inplace=True)\n# col for col iterates over the list training.columns with the variable col and adds it to the resulting list if col is null\n#analyze and drop Rows/Columns with Null values\n#inplace: It is a boolean which makes the changes in data frame itself if True.\n#axis: axis takes int or string value for rows/columns. Input can be 0 or 1 for Integer and ‘index’ or ‘columns’ for String.","metadata":{"id":"l8cy05i_B7Mz","execution":{"iopub.status.busy":"2022-04-28T15:27:23.144817Z","iopub.execute_input":"2022-04-28T15:27:23.145120Z","iopub.status.idle":"2022-04-28T15:27:24.726918Z","shell.execute_reply.started":"2022-04-28T15:27:23.145084Z","shell.execute_reply":"2022-04-28T15:27:24.725793Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"filling the missing values with mode values for categorical features and with median for continuous features.\nThe columns with object dtype are the possible categorical features in your dataset.","metadata":{}},{"cell_type":"code","source":"#filling null alues with mean for continuous variables\nfor i in training.columns:\n    if training[i].dtypes=='int64' or training[i].dtypes=='float64':   \n        training[i].fillna(training[i].mean(),inplace=True)","metadata":{"id":"WBo-2f0CCDf5","execution":{"iopub.status.busy":"2022-04-28T15:27:24.728015Z","iopub.execute_input":"2022-04-28T15:27:24.728216Z","iopub.status.idle":"2022-04-28T15:27:26.071511Z","shell.execute_reply.started":"2022-04-28T15:27:24.728190Z","shell.execute_reply":"2022-04-28T15:27:26.070738Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#filling null alues with mode for categorical variables\nfor i in training.columns:\n    if training[i].dtypes=='object':     \n        training[i].fillna(training[i].mode()[0],inplace=True)","metadata":{"id":"bMpBwxlsCEia","execution":{"iopub.status.busy":"2022-04-28T15:27:26.072600Z","iopub.execute_input":"2022-04-28T15:27:26.072826Z","iopub.status.idle":"2022-04-28T15:27:27.455654Z","shell.execute_reply.started":"2022-04-28T15:27:26.072796Z","shell.execute_reply":"2022-04-28T15:27:27.454824Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#The columns with object dtype are the possible categorical features in your dataset.\ncatagorical_cols = ['id_12','id_15', 'id_16', 'id_23', \n            'id_27', 'id_28', 'id_29','id_30', 'id_31', 'id_33', 'id_34', 'id_35', \n            'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']","metadata":{"id":"Dmmv4C57Cm7k","execution":{"iopub.status.busy":"2022-04-28T15:27:27.456840Z","iopub.execute_input":"2022-04-28T15:27:27.457494Z","iopub.status.idle":"2022-04-28T15:27:27.462078Z","shell.execute_reply.started":"2022-04-28T15:27:27.457463Z","shell.execute_reply":"2022-04-28T15:27:27.461358Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Data Transformation**","metadata":{}},{"cell_type":"markdown","source":"We are applying label encoding on all the categorical features. For example, ‘Credit_card’ can be assigned as 0, ‘Debit_Card’ can be assigned as 1 and ‘Others’ can be assigned as 2.\n\nThe fit method is calculating the mean and variance of each of the features present in our data. The transform method is transforming all the features using the respective mean and variance.","metadata":{}},{"cell_type":"code","source":"#Label encoder can be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n# Basically label encoding is used to convert non-numerical labels such as device type , defice info to numerical labels as 0 , 1 for e.g\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in catagorical_cols:\n  if i in training.columns:\n    training[i] = le.fit_transform(training[i].astype(str).values) ##Fit label encoder of target values and return encoded labels.","metadata":{"id":"VTtsfRPTCvCP","execution":{"iopub.status.busy":"2022-04-28T15:27:27.463061Z","iopub.execute_input":"2022-04-28T15:27:27.463312Z","iopub.status.idle":"2022-04-28T15:27:34.873275Z","shell.execute_reply.started":"2022-04-28T15:27:27.463286Z","shell.execute_reply":"2022-04-28T15:27:34.872464Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"y= training['isFraud']\nprint(y.shape)\n","metadata":{"id":"Qn2FPfIPC89-","outputId":"b6bec212-7c91-480d-dd66-7779cac06caa","execution":{"iopub.status.busy":"2022-04-28T15:27:34.874253Z","iopub.execute_input":"2022-04-28T15:27:34.874721Z","iopub.status.idle":"2022-04-28T15:27:34.878488Z","shell.execute_reply.started":"2022-04-28T15:27:34.874694Z","shell.execute_reply":"2022-04-28T15:27:34.877998Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"x = training.drop(['isFraud','TransactionID','TransactionDT'],axis=1)\nprint(x.shape)","metadata":{"id":"9lRdHF4WGkNr","outputId":"0a76327d-9bcb-4853-b020-10d75066e50a","execution":{"iopub.status.busy":"2022-04-28T15:27:34.879448Z","iopub.execute_input":"2022-04-28T15:27:34.879627Z","iopub.status.idle":"2022-04-28T15:27:35.188658Z","shell.execute_reply.started":"2022-04-28T15:27:34.879602Z","shell.execute_reply":"2022-04-28T15:27:35.188208Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"> The dataset has a highly imbalanced class as we have seen above. There are only 3.5 % fraud cases in our dataset and 96.5% non-fraud cases. We have a ratio of 96.5: 3.5 in our original dataset, we have to divide the dataset for train and test in such a way that both the classes are present in the same proportion in both train and test set. For maintaining the same proportion in the train and test set we have used stratified sampling. We are using 70% of the dataset for training and 30% for testing.\n\n> The features are the descriptive attributes, and the label is what you're attempting to predict or forecast. Here the feature is x and label is y\n\n> here we have to analyze our data whether it is fraud or not whch is available in isFraud so isFraud column will become label of the prediction and rest other columns will be features on which we will be analyzing our model except for transction id and transaction date which is of no relevance here so we will remove both in x.","metadata":{}},{"cell_type":"markdown","source":"The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,stratify = y,test_size = 0.3, random_state=1)","metadata":{"id":"LkjtZ1iYDLFo","execution":{"iopub.status.busy":"2022-04-28T15:27:35.189607Z","iopub.execute_input":"2022-04-28T15:27:35.189856Z","iopub.status.idle":"2022-04-28T15:27:38.261338Z","shell.execute_reply.started":"2022-04-28T15:27:35.189834Z","shell.execute_reply":"2022-04-28T15:27:38.260194Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"train_test_split Split arrays or matrices into random train and test subsets. (*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None) -> list[Any | ndarray | list]\n","metadata":{}},{"cell_type":"code","source":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:27:38.262684Z","iopub.execute_input":"2022-04-28T15:27:38.262915Z","iopub.status.idle":"2022-04-28T15:27:38.268450Z","shell.execute_reply.started":"2022-04-28T15:27:38.262850Z","shell.execute_reply":"2022-04-28T15:27:38.267553Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# **Logistic Regression**","metadata":{}},{"cell_type":"markdown","source":"High ROC simply means the probability of a randomly chosen positive example is indeed positive. High ROC also means your algorithm does a good job at ranking the test data, with most negative cases at one end of a scale and positive cases at the other.","metadata":{}},{"cell_type":"code","source":"#fitting the logistic regression on training set\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(random_state=1)                        \nmodel.fit(x_train,y_train)#Fit the model according to the given training data.","metadata":{"id":"m5ohQ1a9Dcie","outputId":"526e8891-14f4-421b-9833-df8142659da8","execution":{"iopub.status.busy":"2022-04-28T15:27:38.269590Z","iopub.execute_input":"2022-04-28T15:27:38.269762Z","iopub.status.idle":"2022-04-28T15:27:59.212592Z","shell.execute_reply.started":"2022-04-28T15:27:38.269740Z","shell.execute_reply":"2022-04-28T15:27:59.211984Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"predict = model.predict(x_test)  #predicting the class of test labels i.e x","metadata":{"id":"n4qio6vUDgb2","execution":{"iopub.status.busy":"2022-04-28T15:27:59.213790Z","iopub.execute_input":"2022-04-28T15:27:59.214153Z","iopub.status.idle":"2022-04-28T15:27:59.481060Z","shell.execute_reply.started":"2022-04-28T15:27:59.214122Z","shell.execute_reply":"2022-04-28T15:27:59.480371Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.score(x_train,y_train)      #checking the accuracy score for training by Returning the mean accuracy on the given train data and labels.","metadata":{"id":"9OdC4uPoDhdA","outputId":"68b5d272-3ea5-4a82-823a-a3d3b89620aa","execution":{"iopub.status.busy":"2022-04-28T15:27:59.482099Z","iopub.execute_input":"2022-04-28T15:27:59.482314Z","iopub.status.idle":"2022-04-28T15:28:00.497953Z","shell.execute_reply.started":"2022-04-28T15:27:59.482287Z","shell.execute_reply":"2022-04-28T15:28:00.497340Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.score(x_test,y_test)        #checking the accuracy score for testing by Returning the mean accuracy on the given test data and labels.","metadata":{"id":"JxydiDRBDoRN","outputId":"5c86db33-7923-4be7-d4fb-eba86ca3103a","execution":{"iopub.status.busy":"2022-04-28T15:28:00.498998Z","iopub.execute_input":"2022-04-28T15:28:00.499200Z","iopub.status.idle":"2022-04-28T15:28:00.672851Z","shell.execute_reply.started":"2022-04-28T15:28:00.499170Z","shell.execute_reply":"2022-04-28T15:28:00.672283Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"After applying our lgistic regression model we saw that the model was giving accuracy of 96.4 % both for training and testing. But this was a problem here as discussed above we cannot go with accuracy score as we can get a high accuracy score even if our model predicts all the fraud cases as non-fraud. Instead of checking the accuracy score we will see roc_auc_score here.","metadata":{}},{"cell_type":"markdown","source":"The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. Roc & Auc curves are performance measurements for binary classification problems at various thresholds settings. It tells us how much the model is capable of distinguishing between the classes. Higher the Auc, the better the model at predicting 0s as 0 and 1s as 1.","metadata":{}},{"cell_type":"code","source":"# predict probabilities\nlr_probs = model.predict_proba(x_test)\nprint(lr_probs[16])\nprint(lr_probs[15])\nprint(lr_probs[14])","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:28:00.674057Z","iopub.execute_input":"2022-04-28T15:28:00.674293Z","iopub.status.idle":"2022-04-28T15:28:00.839725Z","shell.execute_reply.started":"2022-04-28T15:28:00.674252Z","shell.execute_reply":"2022-04-28T15:28:00.838142Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# keep probabilities for the positive outcome only\noutcome = lr_probs[:,1] ","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:28:00.843145Z","iopub.execute_input":"2022-04-28T15:28:00.843363Z","iopub.status.idle":"2022-04-28T15:28:00.849334Z","shell.execute_reply.started":"2022-04-28T15:28:00.843335Z","shell.execute_reply":"2022-04-28T15:28:00.848718Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for j in np.array([0.14,0.15,0.16]):\n  predict = np.zeros(len(outcome))\n  for i in range(len(outcome)):\n    if outcome[i] > j:\n      predict[i] = 1\n    else:\n      predict[i] = 0","metadata":{"execution":{"iopub.status.busy":"2022-04-28T15:28:00.852305Z","iopub.execute_input":"2022-04-28T15:28:00.852629Z","iopub.status.idle":"2022-04-28T15:28:01.041521Z","shell.execute_reply.started":"2022-04-28T15:28:00.852600Z","shell.execute_reply":"2022-04-28T15:28:01.040424Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,predict)      #checking roc_auc_score","metadata":{"id":"Rnutyk6ZD1b7","outputId":"5aa19dfb-b6d8-4381-c72f-477f362de187","execution":{"iopub.status.busy":"2022-04-28T15:28:01.042906Z","iopub.execute_input":"2022-04-28T15:28:01.043133Z","iopub.status.idle":"2022-04-28T15:28:01.070829Z","shell.execute_reply.started":"2022-04-28T15:28:01.043102Z","shell.execute_reply":"2022-04-28T15:28:01.069830Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"After applying logistic regression we only got 0.51 auc_score which is very less. Our main motive was to predict most of the fraud cases as fraud.","metadata":{}}]}